---
title: Fulbright NÃ´m Library RAG
emoji: ğŸ“š
colorFrom: blue
colorTo: indigo
sdk: docker
pinned: false
---

# Poetry Chef assistant

A full-stack assistant that lets readers explore historical Vietnamese culinary actions. The backend uses a FastAPI pipeline to pick random historical cooking context and uses OpenAI chat completions to "re-cook" user emotions into poetic recipes.

---

## Highlights

- **Poetic Recipe Generation** â€“ Transforms abstract emotions into culinary metaphors based on historical Vietnamese texts.
- **Random Context Selection** â€“ Picks a unique historical recipe from a curated CSV database for every interaction.
- **Luxurious frontend** â€“ Custom Vite/React UI with aurora-inspired theming, highlight animations, and smooth scrolling.
- **Simplified Stack** â€“ Minimalist backend with direct CSV parsing and OpenAI integration, no heavy vector databases required.

---

## Architecture Overview

```text
â”œâ”€â”€ app/                # FastAPI service and Poetry Chef pipeline
â”‚   â”œâ”€â”€ main.py         # API entry-point (`/ask` endpoint)
â”‚   â”œâ”€â”€ rag/            # Prompting and random selection logic
â”‚   â”œâ”€â”€ deps.py         # Dependency wiring
â”‚   â””â”€â”€ settings.py     # Pydantic settings (reads `config/.env`)
â”œâ”€â”€ data/               # Curated historical recipe database (CSV)
â”œâ”€â”€ frontend/           # Vite + React spa (chat experience)
â”‚   â”œâ”€â”€ src/App.tsx     # Main UI logic
â”‚   â”œâ”€â”€ src/client.ts   # API fetch client
â”‚   â””â”€â”€ src/styles.css  # Theme and animations
â”œâ”€â”€ config/
â”‚   â”œâ”€â”€ .env.example    # Sample configuration
â”‚   â””â”€â”€ .env            # Your actual secrets (ignored from VCS)
â”œâ”€â”€ requirements.txt    # Backend dependencies
â””â”€â”€ README.md           # You are here
```

---

## Prerequisites

- **Python 3.10+** (tested with CPython, conda or venv recommended)
- **Node.js 18+** and npm for the frontend
- An **OpenAI API key** (or compatible endpoint) with access to the configured chat model
- Optional: GPU-enabled PyTorch environment if you plan to run large rerankers locally

---

## Quick Start

### 1. Clone & prepare environments

```powershell
# In PowerShell
cd AI-chatbot-2
python -m venv .venv
.\.venv\Scripts\Activate.ps1
pip install -r requirements.txt
```

```powershell
cd frontend
npm install
cd ..
```

### 2. Configure environment variables

```powershell
Copy-Item config/.env.example config/.env
notepad config/.env    # fill in OPENAI_API_KEY and other options
```

Key variables (see `config/.env.example` for full list):

- `OPENAI_API_KEY` â€“ required for chat completions
- `EMBEDDING_MODEL` â€“ Hugging Face embedding checkpoint (default `BAAI/bge-m3`)
- `RERANK_MODEL` â€“ Optional cross-encoder reranker (default `BAAI/bge-reranker-large`)
- `CHAT_MODEL` â€“ OpenAI chat model identifier (default `gpt-4o-mini`)
- `PERSIST_DIR` â€“ Vector-store folder (defaults to `chroma_db`)
- Retrieval knobs: `RETRIEVER_K`, `POOL_SIZE`, `RERANK_TOP_K`, `CHUNK_SIZE`, `CHUNK_OVERLAP`

### 3. Run the FastAPI backend

```powershell
uvicorn app.main:app --reload --port 8000
```

The service exposes:

- `GET /health` â€“ Basic health probe
- `POST /ask` â€“ Accepts an `AskRequest` (question, top_k, pool_size, rerank toggle) and returns an `AskResponse` with answer text plus ordered source chunks.

### 5. Start the React frontend

```powershell
cd frontend
npm run dev
```

Open the printed URL (typically <http://localhost:5173>). Ensure the backend is reachable at <http://localhost:8000>; set `VITE_API_BASE` in `frontend/.env` if you deploy the API elsewhere.

---

## User Experience

- Prompt the model via the textarea and submit to trigger `/ask`.
- The interface auto-scrolls to the freshly generated answer and briefly highlights the response card.
- Citations list every retrieved chunk; use **Xem trÃ­ch Ä‘oáº¡n** to expand the passage or **Xem nhanh** to sync the built-in PDF preview.
- The preview pane loads the best-matching document page, with a convenient â€œMá»Ÿ toÃ n vÄƒnâ€ button to open the full PDF in a new tab.

---

## Development Workflow

### Testing & Troubleshooting

- **Backend imports failing**: run commands from the project root and export `PYTHONPATH` as shown above.
- **Missing models**: the first ingestion may download multi-hundred MB checkpoints; keep the process running until it completes.
- **Frontend network errors**: verify the backend base URL, or adjust `frontend/vite.config.ts` proxy if you introduce HTTPS or a different port.
- **Refresh vector store**: `Remove-Item chroma_db -Recurse -Force` then re-run `scripts.ingest`.

### Formatting & Linting

- Backend adheres to standard `black`/`ruff` style (optional but recommended).
- Frontend uses TypeScript + eslint presets from Vite (configurable via `frontend/eslint.config.js`).

### Useful npm scripts (`frontend/package.json`)

- `npm run dev` â€“ Vite dev server with hot reload
- `npm run build` â€“ Production bundle (outputs to `frontend/dist/`)
- `npm run preview` â€“ Serve the production build locally

---

## Deployment Notes

- Backend can be containerized with Uvicorn + Gunicorn (check `uvicorn-gunicorn` images) and served behind a reverse proxy.
- Persist `chroma_db/` on durable storage (e.g., Azure Files, AWS EFS, or local volume) so embeddings survive restarts.
- Inject `OPENAI_API_KEY` and other secrets via your orchestration platform (GitHub Actions, Azure App Service, etc.).
- Build the frontend with `npm run build` and host the static assets on a CDN or alongside the FastAPI app using `StaticFiles` if desired.

---

## Roadmap Ideas

1. Add streaming responses for incremental answer rendering.
2. Introduce citation-based highlighting inside the PDF viewer.
3. Expand ingestion to handle Markdown/HTML scraping pipelines.
4. Implement authentication and usage analytics for multi-user deployments.

